"""
English Quiz System using RAG
RAGã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰è‹±æ–‡ã‚’æŠ½å‡ºã—ã€å’Œè¨³ã‚’æ¡ç‚¹ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 
"""
import sys
import os
import re
import random
from difflib import SequenceMatcher
import numpy as np
from collections import Counter
from janome.tokenizer import Tokenizer

from simple_vector_store import SimpleVectorStore
from simple_embeddings import SimpleEmbeddings


class EnglishQuizSystem:
    def __init__(self):
        self.vector_store = SimpleVectorStore()
        self.embeddings = SimpleEmbeddings()
        self.tokenizer = Tokenizer()
        self.current_question = None
        self.score_history = []

    def extract_english_sentences(self, text, min_length=50, max_length=200):
        sentences = re.split(r'[.!?]\s+', text)

        english_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()

            if len(sentence) < min_length or len(sentence) > max_length:
                continue

            ascii_chars = sum(1 for c in sentence if ord(c) < 128)
            if ascii_chars / len(sentence) > 0.7:
                english_sentences.append(sentence)

        return english_sentences

    def get_random_english_question(self):
        all_docs = self.vector_store.get_all_documents()

        if not all_docs:
            return None

        random.shuffle(all_docs)

        for doc in all_docs:
            doc_text = doc['text']

            if 'EN:' in doc_text and 'JP:' in doc_text:
                lines = doc_text.split('\n')
                english_line = None
                japanese_line = None

                for line in lines:
                    if line.startswith('EN:'):
                        english_line = line.replace('EN:', '').strip()
                    elif line.startswith('JP:'):
                        japanese_line = line.replace('JP:', '').strip()

                if english_line and japanese_line:
                    self.current_question = {
                        'english': english_line,
                        'japanese': japanese_line,
                        'source': doc.get('metadata', {}).get('source', 'Unknown'),
                        'doc_id': doc['id']
                    }
                    return self.current_question
            else:
                sentences = self.extract_english_sentences(doc_text)
                if sentences:
                    selected_sentence = random.choice(sentences)
                    self.current_question = {
                        'english': selected_sentence,
                        'source': doc.get('metadata', {}).get('source', 'Unknown'),
                        'doc_id': doc['id']
                    }
                    return self.current_question

        return None

    def calculate_similarity(self, text1, text2):
        text1 = text1.lower().strip()
        text2 = text2.lower().strip()

        text1 = re.sub(r'[ã€ã€‚ï¼ï¼Ÿ\s]+', '', text1)
        text2 = re.sub(r'[ã€ã€‚ï¼ï¼Ÿ\s]+', '', text2)

        return SequenceMatcher(None, text1, text2).ratio()

    def tokenize_japanese(self, text):
        excluded_pos = {
            'åŠ©è©',
            'åŠ©å‹•è©',
            'è¨˜å·',
            'æ¥ç¶šè©',
            'æ¥é ­è©',
            'éè‡ªç«‹'
        }

        excluded_words = {
            'ã§ã™', 'ã¾ã™', 'ã§ã‚ã‚‹', 'ã ', 'ãŸ', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹',
            'ã›ã‚‹', 'ã•ã›ã‚‹', 'ãªã„', 'ã¬', 'ã†', 'ã‚ˆã†'
        }

        words = []
        tokens = self.tokenizer.tokenize(text)

        for token in tokens:
            parts = token.split('\t')
            if len(parts) < 2:
                continue

            surface = parts[0]
            features = parts[1].split(',')

            if len(features) < 1:
                continue

            pos = features[0]

            is_excluded_pos = any(ex in pos for ex in excluded_pos)
            is_excluded_word = surface in excluded_words
            is_too_short = len(surface) <= 1

            if not is_excluded_pos and not is_excluded_word and not is_too_short:
                words.append(surface)

        return words

    def calculate_word_overlap(self, text1, text2):
        words1 = self.tokenize_japanese(text1)
        words2 = self.tokenize_japanese(text2)

        counter1 = Counter(words1)
        counter2 = Counter(words2)

        common = counter1 & counter2
        total = counter2

        overlap = sum(common.values())
        total_count = sum(total.values())

        return (overlap / total_count) if total_count > 0 else 0, words1, words2, common

    def calculate_vector_similarity(self, text1, text2):
        vec1 = self.embeddings.encode_single(text1)
        vec2 = self.embeddings.encode_single(text2)

        vec1 = np.array(vec1)
        vec2 = np.array(vec2)

        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0

        return dot_product / (norm1 * norm2)

    def score_translation(self, user_translation, current_question=None, debug=False):
        scoring_details = {
            'steps': [],
            'normalized_user': '',
            'normalized_reference': '',
            'raw_similarity': 0.0
        }

        if not user_translation.strip():
            return {
                'score': 0,
                'feedback': 'å›ç­”ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚',
                'grade': 'F',
                'reference_translation': None,
                'scoring_details': scoring_details
            }

        reference_translation = None
        best_score = 0

        scoring_details['steps'].append('ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: æ­£è§£ã®å’Œè¨³ã‚’å–å¾—')

        if current_question and 'japanese' in current_question:
            reference_translation = current_question['japanese']
            scoring_details['steps'].append(f'âœ… æ­£è§£: {reference_translation}')

            scoring_details['steps'].append('\nğŸ”¤ ã‚¹ãƒ†ãƒƒãƒ—2: å½¢æ…‹ç´ è§£æï¼ˆå˜èªåˆ†å‰²ãƒ»åŠ©è©é™¤å¤–ï¼‰')

            word_overlap, user_words, ref_words, common_words = self.calculate_word_overlap(
                user_translation, reference_translation
            )

            scoring_details['user_words'] = user_words
            scoring_details['ref_words'] = ref_words
            scoring_details['common_words'] = list(common_words.elements())
            scoring_details['word_overlap'] = word_overlap

            scoring_details['steps'].append('â€» åŠ©è©ï¼ˆã¯ã€ãŒã€ã‚’ã€ã«ç­‰ï¼‰ã¯é™¤å¤–ã—ã¦è©•ä¾¡')
            scoring_details['steps'].append(f'ã‚ãªãŸã®å†…å®¹èªæ•°: {len(user_words)}å€‹')
            scoring_details['steps'].append(f'æ­£è§£ã®å†…å®¹èªæ•°: {len(ref_words)}å€‹')
            scoring_details['steps'].append(f'ä¸€è‡´ã—ãŸå˜èªæ•°: {len(scoring_details["common_words"])}å€‹')
            scoring_details['steps'].append(f'å˜èªä¸€è‡´ç‡: {word_overlap:.4f} ({word_overlap*100:.2f}%)')

            if scoring_details['common_words']:
                words_preview = 'ã€'.join(scoring_details['common_words'][:15])
                if len(scoring_details['common_words']) > 15:
                    words_preview += f'... (ä»–{len(scoring_details["common_words"])-15}å€‹)'
                scoring_details['steps'].append(f'ä¸€è‡´ã—ãŸå˜èª: {words_preview}')

            scoring_details['steps'].append('\nğŸ§® ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦è¨ˆç®—ï¼ˆRAGæ–¹å¼ï¼‰')

            vector_sim = self.calculate_vector_similarity(user_translation, reference_translation)
            scoring_details['vector_similarity'] = vector_sim

            scoring_details['steps'].append(f'ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰: {vector_sim:.4f} ({vector_sim*100:.2f}%)')
            scoring_details['steps'].append('â€» æ–‡ç« å…¨ä½“ã®æ„å‘³çš„ãªè¿‘ã•ã‚’æ¸¬å®š')

            scoring_details['steps'].append('\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—4: æ–‡å­—åˆ—é¡ä¼¼åº¦è¨ˆç®—')

            user_normalized = re.sub(r'[ã€ã€‚ï¼ï¼Ÿ\s]+', '', user_translation.lower().strip())
            ref_normalized = re.sub(r'[ã€ã€‚ï¼ï¼Ÿ\s]+', '', reference_translation.lower().strip())

            scoring_details['normalized_user'] = user_normalized
            scoring_details['normalized_reference'] = ref_normalized

            matcher = SequenceMatcher(None, user_normalized, ref_normalized)
            string_sim = matcher.ratio()
            scoring_details['string_similarity'] = string_sim

            total_len = len(ref_normalized)
            matched_len = 0
            common_parts = []
            diff_parts = []

            for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                if tag == 'equal':
                    part = user_normalized[i1:i2]
                    common_parts.append(part)
                    matched_len += len(part)
                    diff_parts.append(('match', part, part))
                elif tag == 'replace':
                    diff_parts.append(('replace', user_normalized[i1:i2], ref_normalized[j1:j2]))
                elif tag == 'delete':
                    diff_parts.append(('delete', user_normalized[i1:i2], ''))
                elif tag == 'insert':
                    diff_parts.append(('insert', '', ref_normalized[j1:j2]))

            scoring_details['diff_parts'] = diff_parts
            scoring_details['matched_length'] = matched_len
            scoring_details['total_length'] = total_len

            scoring_details['steps'].append(f'æ–‡å­—åˆ—é¡ä¼¼åº¦: {string_sim:.4f} ({string_sim*100:.2f}%)')

            scoring_details['steps'].append('\nğŸ¯ ã‚¹ãƒ†ãƒƒãƒ—5: ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—')

            weight_vector = 0.40
            weight_word = 0.40
            weight_string = 0.20

            best_score = (
                vector_sim * weight_vector +
                word_overlap * weight_word +
                string_sim * weight_string
            )

            scoring_details['raw_similarity'] = best_score
            scoring_details['weights'] = {
                'vector': weight_vector,
                'word': weight_word,
                'string': weight_string
            }

            scoring_details['steps'].append(f'è¨ˆç®—å¼: (ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ Ã— {weight_vector}) + (å˜èªä¸€è‡´ç‡ Ã— {weight_word}) + (æ–‡å­—åˆ—é¡ä¼¼åº¦ Ã— {weight_string})')
            scoring_details['steps'].append(f'= ({vector_sim:.3f} Ã— {weight_vector}) + ({word_overlap:.3f} Ã— {weight_word}) + ({string_sim:.3f} Ã— {weight_string})')
            scoring_details['steps'].append(f'= {vector_sim*weight_vector:.3f} + {word_overlap*weight_word:.3f} + {string_sim*weight_string:.3f}')
            scoring_details['steps'].append(f'= {best_score:.4f}')

        else:
            scoring_details['steps'].append('âš ï¸ æ­£è§£ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆæ¤œç´¢ä¸­...ï¼‰')
            english_text = current_question.get('english', '') if current_question else ''
            all_docs = self.vector_store.get_all_documents()

            for doc in all_docs:
                doc_text = doc['text']

                if 'EN:' in doc_text and 'JP:' in doc_text:
                    lines = doc_text.split('\n')
                    english_line = None
                    japanese_line = None

                    for line in lines:
                        if line.startswith('EN:'):
                            english_line = line.replace('EN:', '').strip()
                        elif line.startswith('JP:'):
                            japanese_line = line.replace('JP:', '').strip()

                    if english_line and japanese_line:
                        en_normalized = english_line.strip().lower()
                        query_normalized = english_text.strip().lower()

                        if en_normalized == query_normalized:
                            similarity = self.calculate_similarity(user_translation, japanese_line)

                            if similarity > best_score:
                                best_score = similarity
                                reference_translation = japanese_line

        score = int(best_score * 100)
        scoring_details['steps'].append(f'\nğŸ¯ æœ€çµ‚ã‚¹ã‚³ã‚¢: {score}ç‚¹')

        if score >= 90:
            grade = 'S'
            feedback = 'ç´ æ™´ã‚‰ã—ã„ï¼ã»ã¼å®Œç’§ãªç¿»è¨³ã§ã™ã€‚'
        elif score >= 80:
            grade = 'A'
            feedback = 'éå¸¸ã«è‰¯ã„ç¿»è¨³ã§ã™ï¼'
        elif score >= 70:
            grade = 'B'
            feedback = 'è‰¯ã„ç¿»è¨³ã§ã™ã€‚ã„ãã¤ã‹æ”¹å–„ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚'
        elif score >= 60:
            grade = 'C'
            feedback = 'ã¾ãšã¾ãšã§ã™ã€‚ã‚‚ã†å°‘ã—æ­£ç¢ºã«ç¿»è¨³ã—ã¾ã—ã‚‡ã†ã€‚'
        elif score >= 40:
            grade = 'D'
            feedback = 'æ„å‘³ã¯ä¼ã‚ã£ã¦ã„ã¾ã™ãŒã€æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚'
        else:
            grade = 'F'
            feedback = 'ç¿»è¨³ã®ç²¾åº¦ãŒä½ã„ã§ã™ã€‚å†åº¦ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã—ã¾ã—ã‚‡ã†ã€‚'

        result = {
            'score': score,
            'grade': grade,
            'feedback': feedback,
            'english': current_question.get('english', '') if current_question else '',
            'reference_translation': reference_translation,
            'scoring_details': scoring_details
        }

        self.score_history.append(result)

        return result

    def extract_japanese_sentences(self, text):
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)

        japanese_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()

            if len(sentence) < 10:
                continue

            japanese_chars = sum(1 for c in sentence if '\u3040' <= c <= '\u309F' or '\u30A0' <= c <= '\u30FF' or '\u4E00' <= c <= '\u9FFF')

            if japanese_chars / len(sentence) > 0.3:
                japanese_sentences.append(sentence)

        return japanese_sentences

    def get_statistics(self):
        if not self.score_history:
            return None

        scores = [result['score'] for result in self.score_history]

        return {
            'total_questions': len(self.score_history),
            'average_score': sum(scores) / len(scores),
            'highest_score': max(scores),
            'lowest_score': min(scores),
            'grade_distribution': self._calculate_grade_distribution()
        }

    def _calculate_grade_distribution(self):
        grades = {}
        for result in self.score_history:
            grade = result['grade']
            grades[grade] = grades.get(grade, 0) + 1
        return grades


def main():
    print("=" * 60)
    print("ğŸ“ English Quiz System - RAGãƒ™ãƒ¼ã‚¹ã®è‹±èªå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)

    quiz = EnglishQuizSystem()

    print(f"\nğŸ“š åˆ©ç”¨å¯èƒ½ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(quiz.vector_store.documents)}")

    if len(quiz.vector_store.documents) == 0:
        print("\nâš ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        print("å…ˆã«PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’RAGã‚·ã‚¹ãƒ†ãƒ ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
        return

    while True:
        print("\n" + "=" * 60)
        question = quiz.get_random_english_question()

        if not question:
            print("âŒ è‹±æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            break

        print("\nğŸ“– ä»¥ä¸‹ã®è‹±æ–‡ã‚’å’Œè¨³ã—ã¦ãã ã•ã„:")
        print("-" * 60)
        print(question['english'])
        print("-" * 60)
        print(f"å‡ºå…¸: {os.path.basename(question['source'])}")

        user_translation = input("\nâœï¸ ã‚ãªãŸã®å’Œè¨³ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()

        if user_translation.lower() in ['quit', 'exit', 'q']:
            print("\nğŸ‘‹ çµ‚äº†ã—ã¾ã™ã€‚")
            break

        print("\nâ³ æ¡ç‚¹ä¸­...")
        result = quiz.score_translation(user_translation, question['english'])

        print("\n" + "=" * 60)
        print("ğŸ“Š æ¡ç‚¹çµæœ")
        print("=" * 60)
        print(f"ã‚¹ã‚³ã‚¢: {result['score']}ç‚¹")
        print(f"ã‚°ãƒ¬ãƒ¼ãƒ‰: {result['grade']}")
        print(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: {result['feedback']}")
        print("=" * 60)

        continue_choice = input("\næ¬¡ã®å•é¡Œã«é€²ã¿ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()

        if continue_choice != 'y':
            break

    stats = quiz.get_statistics()
    if stats:
        print("\n" + "=" * 60)
        print("ğŸ“ˆ å­¦ç¿’çµ±è¨ˆ")
        print("=" * 60)
        print(f"ç·å•é¡Œæ•°: {stats['total_questions']}")
        print(f"å¹³å‡ã‚¹ã‚³ã‚¢: {stats['average_score']:.1f}ç‚¹")
        print(f"æœ€é«˜ã‚¹ã‚³ã‚¢: {stats['highest_score']}ç‚¹")
        print(f"æœ€ä½ã‚¹ã‚³ã‚¢: {stats['lowest_score']}ç‚¹")
        print(f"\nã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ:")
        for grade, count in sorted(stats['grade_distribution'].items()):
            print(f"  {grade}: {count}å›")
        print("=" * 60)


if __name__ == "__main__":
    main()
