"""
æ—¥æœ¬èªå¯¾å¿œSentence-BERTãƒ¢ãƒ‡ãƒ«ï¼ˆæ”¹å–„ç‰ˆï¼‰
"""
import numpy as np
from typing import List
from sentence_transformers import SentenceTransformer
import re

class SimpleEmbeddings:
    def __init__(self):
        print("ğŸ“¦ æ—¥æœ¬èªå¯¾å¿œAIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")

        # æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        try:
            model_name = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'
            self.model = SentenceTransformer(model_name)
            print(f"âœ… {model_name}ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
        except:
            model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
            self.model = SentenceTransformer(model_name)
            print(f"âœ… {model_name}ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

        self.dimension = 768

    def preprocess_japanese(self, text):
        """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
        # åŠ©è©ã‚’å‰Šé™¤ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        particles = ['ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã¸', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§', 'ã§', 'ã®']
        for p in particles:
            text = text.replace(p, ' ')
        # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def encode(self, texts):
        # å‰å‡¦ç†ã‚’é©ç”¨
        processed_texts = [self.preprocess_japanese(t) for t in texts]
        return self.model.encode(processed_texts)

    def encode_single(self, text):
        processed = self.preprocess_japanese(text)
        return self.model.encode([processed])[0].tolist()
