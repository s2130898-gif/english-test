"""
DistilBERT ãƒ™ãƒ¼ã‚¹ã®AIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
å¤šè¨€èªžå¯¾å¿œï¼ˆæ—¥æœ¬èªžãƒ»è‹±èªžï¼‰ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
"""
import numpy as np
from typing import List
import torch
from transformers import AutoTokenizer, AutoModel

class SimpleEmbeddings:
    """DistilBERTã‚’ä½¿ã£ãŸAIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«"""

    def __init__(self):
        print("ðŸ“¦ AIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        print("âš ï¸ åˆå›žèµ·å‹•æ™‚ã¯3-4åˆ†ã‹ã‹ã‚Šã¾ã™...")

        model_name = 'distilbert-base-multilingual-cased'

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)

        self.model.eval()

        self.dimension = 768

        print("âœ… DistilBERTå¤šè¨€èªžãƒ¢ãƒ‡ãƒ« (768æ¬¡å…ƒ) ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
        print("ðŸ¤– ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹AIæŽ¡ç‚¹ãŒæœ‰åŠ¹ã§ã™")

    def encode(self, texts: List[str]) -> np.ndarray:
        embeddings = []

        with torch.no_grad():
            for text in texts:
                inputs = self.tokenizer(
                    text,
                    return_tensors='pt',
                    truncation=True,
                    max_length=512,
                    padding=True
                )

                outputs = self.model(**inputs)

                embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()

                embeddings.append(embedding)

        return np.array(embeddings)

    def encode_single(self, text: str) -> List[float]:
        return self.encode([text])[0].tolist()
