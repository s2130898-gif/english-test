"""
DistilBERT ãƒ™ãƒ¼ã‚¹ã®AIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œç‰ˆï¼‰
"""
import numpy as np
from typing import List
import torch
from transformers import AutoTokenizer, AutoModel

class SimpleEmbeddings:
  def __init__(self):
      print("ðŸ“¦ AIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
      print("âš ï¸ åˆå›žèµ·å‹•æ™‚ã¯3-4åˆ†ã‹ã‹ã‚Šã¾ã™...")

      model_name = 'distilbert-base-multilingual-cased'

      # ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’æ˜Žç¤ºçš„ã«æŒ‡å®š
      self.device = torch.device('cpu')  # CPUã‚’å¼·åˆ¶ä½¿ç”¨

      try:
          self.tokenizer = AutoTokenizer.from_pretrained(model_name)
          self.model = AutoModel.from_pretrained(
              model_name,
              torch_dtype=torch.float32,  # ãƒ‡ãƒ¼ã‚¿åž‹ã‚’æ˜Žç¤º
              device_map=None  # ãƒ‡ãƒã‚¤ã‚¹ãƒžãƒƒãƒ—ã‚’ç„¡åŠ¹åŒ–
          )

          # ãƒ¢ãƒ‡ãƒ«ã‚’CPUã«ç§»å‹•
          self.model = self.model.to(self.device)
          self.model.eval()

      except Exception as e:
          print(f"ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
          # ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã§ãƒªãƒˆãƒ©ã‚¤
          model_name = 'distilbert-base-uncased'
          self.tokenizer = AutoTokenizer.from_pretrained(model_name)
          self.model = AutoModel.from_pretrained(model_name)
          self.model = self.model.to(self.device)
          self.model.eval()

      self.dimension = 768
      print("âœ… DistilBERTå¤šè¨€èªžãƒ¢ãƒ‡ãƒ« (768æ¬¡å…ƒ) ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

  def encode(self, texts: List[str]) -> np.ndarray:
      embeddings = []

      with torch.no_grad():
          for text in texts:
              try:
                  inputs = self.tokenizer(
                      text,
                      return_tensors='pt',
                      truncation=True,
                      max_length=512,
                      padding=True
                  )

                  # å…¥åŠ›ã‚‚CPUã«ç§»å‹•
                  inputs = {k: v.to(self.device) for k, v in inputs.items()}

                  outputs = self.model(**inputs)
                  embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()
                  embeddings.append(embedding)

              except Exception as e:
                  print(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ€ãƒŸãƒ¼ãƒ™ã‚¯ãƒˆãƒ«
                  embeddings.append(np.random.rand(self.dimension))

      return np.array(embeddings)

  def encode_single(self, text: str) -> List[float]:
      return self.encode([text])[0].tolist()
