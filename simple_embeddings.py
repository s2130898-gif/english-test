  """
  DistilBERT ãƒ™ãƒ¼ã‚¹ã®AIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
  å¤šè¨€èªžå¯¾å¿œï¼ˆæ—¥æœ¬èªžãƒ»è‹±èªžï¼‰ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
  """
  import numpy as np
  from typing import List
  import torch
  from transformers import AutoTokenizer, AutoModel

  class SimpleEmbeddings:
      """DistilBERTã‚’ä½¿ã£ãŸAIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«"""

      def __init__(self):
          print("ðŸ“¦ AIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
          print("âš ï¸ åˆå›žèµ·å‹•æ™‚ã¯3-4åˆ†ã‹ã‹ã‚Šã¾ã™...")

          model_name = 'distilbert-base-multilingual-cased'

          self.tokenizer = AutoTokenizer.from_pretrained(model_name)
          self.model = AutoModel.from_pretrained(model_name)

          self.model.eval()

          self.dimension = 768

          print("âœ… DistilBERTå¤šè¨€èªžãƒ¢ãƒ‡ãƒ« (768æ¬¡å…ƒ) ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
          print("ðŸ¤– ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹AIæŽ¡ç‚¹ãŒæœ‰åŠ¹ã§ã™")

      def encode(self, texts: List[str]) -> np.ndarray:
          embeddings = []

          with torch.no_grad():
              for text in texts:
                  inputs = self.tokenizer(
                      text,
                      return_tensors='pt',
                      truncation=True,
                      max_length=512,
                      padding=True
                  )

                  outputs = self.model(**inputs)

                  # Mean poolingã‚’ä½¿ç”¨ï¼ˆæ”¹å–„ç‰ˆï¼‰
                  attention_mask = inputs['attention_mask']
                  hidden_states = outputs.last_hidden_state

                  masked_embeddings = hidden_states * attention_mask.unsqueeze(-1)
                  sum_embeddings = masked_embeddings.sum(dim=1)
                  sum_mask = attention_mask.sum(dim=1, keepdim=True)

                  embedding = (sum_embeddings / sum_mask.clamp(min=1e-9)).squeeze().numpy()

                  embeddings.append(embedding)

          return np.array(embeddings)

      def encode_single(self, text: str) -> List[float]:
          return self.encode([text])[0].tolist()
